{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zunyong01/hello-world/blob/main/lec27_notebook_for_students/swcon425_lec27_notebook_for_students.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8bfa70ec-5c4c-40e8-b923-16f8167e3181",
      "metadata": {
        "id": "8bfa70ec-5c4c-40e8-b923-16f8167e3181"
      },
      "source": [
        "# Lec 27 어텐션 메커니즘 구현하기"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c29bcbe8-a034-43a2-b557-997b03c9882d",
      "metadata": {
        "id": "c29bcbe8-a034-43a2-b557-997b03c9882d"
      },
      "source": [
        "실습에 사용될 패키지를 미리 다운로드해주세요."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "e58f33e8-5dc9-4dd5-ab84-5a011fa11d92",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e58f33e8-5dc9-4dd5-ab84-5a011fa11d92",
        "outputId": "1a1ef4c4-9612-491b-d12a-607ee3f8f7ef"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "파이토치 버전: 2.9.0+cu126\n"
          ]
        }
      ],
      "source": [
        "from importlib.metadata import version\n",
        "\n",
        "print(\"파이토치 버전:\", version(\"torch\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a2a4474d-7c68-4846-8702-37906cf08197",
      "metadata": {
        "id": "a2a4474d-7c68-4846-8702-37906cf08197"
      },
      "source": [
        "이번 실습 시간에는 LLM의 핵심 방법론인 어텐션 메커니즘을 다루겠습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "02a11208-d9d3-44b1-8e0d-0c8414110b93",
      "metadata": {
        "id": "02a11208-d9d3-44b1-8e0d-0c8414110b93"
      },
      "source": [
        "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch03_compressed/01.webp?123\" width=\"1000px\">"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "50e020fd-9690-4343-80df-da96678bef5e",
      "metadata": {
        "id": "50e020fd-9690-4343-80df-da96678bef5e"
      },
      "source": [
        "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch03_compressed/02.webp\" width=\"1000px\">"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ecc4dcee-34ea-4c05-9085-2f8887f70363",
      "metadata": {
        "id": "ecc4dcee-34ea-4c05-9085-2f8887f70363"
      },
      "source": [
        "## 3.1 긴 시퀀스 모델링의 문제점"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a55aa49c-36c2-48da-b1d9-70f416e46a6a",
      "metadata": {
        "id": "a55aa49c-36c2-48da-b1d9-70f416e46a6a"
      },
      "source": [
        "트랜스포머(Transformer)는 원래 기계 번역 성능을 개선하기 위해 고안된 모델입니다.  \n",
        "트랜스포머의 작동 원리를 알아보기 전에, 먼저 번역 task의 특성과 기존 방법론의 한계에 대해 알아보도록 하겠습니다.\n",
        "\n",
        "번역을 수행할 때는 소스 언어와 타깃 언어 사이의 문법 구조 차이를 고려해야 합니다.  \n",
        "서로 다른 언어 간의 문법 차이 때문에 텍스트는 한 단어씩 번역될 수 없습니다.\n",
        "\n",
        "<img src=\"https://github.com/dusdnKR/SWCON425/blob/main/lec27_notebook_for_students/image/lec27_00.png?raw=1\" width=\"1000px\">"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "db03c48a-3429-48ea-9d4a-2e53b0e516b1",
      "metadata": {
        "id": "db03c48a-3429-48ea-9d4a-2e53b0e516b1"
      },
      "source": [
        "트랜스포머 모델이 소개되기 전에는 Seq2Seq 구조의 인코더-디코더 RNN이 기계 번역 작업에 널리 사용되었습니다.  \n",
        "\n",
        "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch03_compressed/04.webp\" width=\"1000px\">\n",
        "\n",
        "<b>인코더(Encoder)</b>는 소스 언어의 토큰 시퀀스를 순차적으로 처리하여 최종적으로 하나의 컨텍스트 벡터를 반환합니다.  \n",
        "<b>디코더(Decoder)</b>는 압축된 정보를 담고 있는 컨텍스트 벡터를 토대로 타겟 언어의 토큰를 하나씩 생성합니다.\n",
        "\n",
        "그림에서 볼 수 있듯, seq2seq 모델은 입력 토큰을 순서대로 처리하여 하나의 벡터를 반환합니다.  \n",
        "때문에 문장이 길어지면 길어질수록 초반에 사용되었던 단어의 의미가 희미해지는 문제가 존재했고, 입력 문장의 길이에 관계없이 모든 정보를 고정된 벡터에 담아내야 한다는 점과, 이전 연산이 진행되어야 다음 연산 결과를 구할 수 있어 병렬 처리가 어렵다는 점이 한계로 작용했습니다.\n",
        "\n",
        "**어텐션 메커니즘**은 이러한 Seq2Seq의 한계를 극복하기 위해 등장했습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3602c585-b87a-41c7-a324-c5e8298849df",
      "metadata": {
        "id": "3602c585-b87a-41c7-a324-c5e8298849df"
      },
      "source": [
        "## 3.2 어텐션 메커니즘으로 데이터 종속성 포착하기"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b6fde64c-6034-421d-81d9-8244932086ea",
      "metadata": {
        "id": "b6fde64c-6034-421d-81d9-8244932086ea"
      },
      "source": [
        "어텐션 메커니즘의 텍스트 생성 디코더도 모든 입력 토큰과 상호작용할 수 있습니다.  \n",
        "seq2seq 모델과 다른 점은 생성하려는 단어와 관계성이 짙은 단어에 더 집중(attention)할 수 있다는 것입니다.\n",
        "\n",
        "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch03_compressed/05.webp\" width=\"1000px\">\n",
        "\n",
        "그럼 어떤 단어에 더 attention해야 하는지 어떻게 알 수 있을까요?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5efe05ff-b441-408e-8d66-cde4eb3397e3",
      "metadata": {
        "id": "5efe05ff-b441-408e-8d66-cde4eb3397e3"
      },
      "source": [
        "## 3.3 셀프 어텐션으로 입력의 서로 다른 부분에 주의를 기울이기"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6d9af516-7c37-4400-ab53-34936d5495a9",
      "metadata": {
        "id": "6d9af516-7c37-4400-ab53-34936d5495a9"
      },
      "source": [
        "### 3.3.1 훈련 가능한 가중치가 없는 간단한 셀프 어텐션 메커니즘"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d269e9f1-df11-4644-b575-df338cf46cdf",
      "metadata": {
        "id": "d269e9f1-df11-4644-b575-df338cf46cdf"
      },
      "source": [
        "셀프 어텐션(self attention)이란, attention 방법론을 동일한 input에 대해 적용하는 것입니다.  \n",
        "이해를 위해 <b>훈련 가능한 가중치가 없는</b> 간단한 버전의 셀프 어텐션을 구현하겠습니다.\n",
        "\n",
        "-  $x^{(1)}$부터 $x^{(T)}$까지 하나의 입력 시퀀스가 주어졌다고 가정해 보겠습니다.\n",
        "    - 입력은 텍스트(예를 들어 \"Your journey starts with one step\"와 같은 문장)이며, x는 각 토큰의 토큰 임베딩입니다.\n",
        "    - 예를 들어 $x^{(1)}$은 단어 \"Your\"를 나타내는 d차원 벡터입니다.\n",
        "- $x^{(1)}$에서 $x^{(T)}$ 사이에 있는 각 입력 시퀀스 원소 $x^{(i)}$에 대한 문맥 벡터(context vector) $z^{(i)}$를 계산하겠습니다. ($z$와 $x$는 차원이 같습니다)\n",
        "    - 문맥 벡터는 어떠한 토큰을 기준으로 하는 전체 문맥 정보를 담고 있는 벡터입니다.\n",
        "    - 문맥 벡터 $z^{(i)}$는 입력 $x^{(1)}$에서 $x^{(T)}$까지에 대한 가중치 합입니다.\n",
        "    - 각각의 x에 곱해지는 가중치는 <b>어텐션 가중치(attention weight)</b>로, x의 기여도를 조절합니다."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fcc7c7a2-b6ab-478f-ae37-faa8eaa8049a",
      "metadata": {
        "id": "fcc7c7a2-b6ab-478f-ae37-faa8eaa8049a"
      },
      "source": [
        "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch03_compressed/07.webp\" width=\"1000px\">"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "01b10344-128d-462a-823f-2178dff5fd58",
      "metadata": {
        "id": "01b10344-128d-462a-823f-2178dff5fd58"
      },
      "source": [
        "구체적으로, 두 번째 입력 $x^{(2)}$에 대한 $z^{(2)}$를 구한다고 가정해보겠습니다.\n",
        "\n",
        "우선 $x^{(2)}$과 $x^{(1)}$부터 $x^{(T)}$ 간의 <b>attention score</b>를 구해야 합니다.  \n",
        "Attention score는 각 토큰 임베딩 벡터 간의 내적을 통해 계산됩니다. 이를 통해 벡터 간의 유사도를 수치화할 수 있습니다.\n",
        "\n",
        "두 번째 입력 토큰을 쿼리로 사용한다면(즉, $q^{(2)} = x^{(2)}$), attention score $\\omega$는 다음과 같이 계산됩니다.\n",
        "- $\\omega_{21} = x^{(1)} x^{(2)\\top} = x^{(1)} q^{(2)\\top}$\n",
        "- $\\omega_{22} = x^{(2)} x^{(2)\\top} = x^{(2)} q^{(2)\\top}$\n",
        "- $\\omega_{23} = x^{(3)} x^{(2)\\top} = x^{(3)} q^{(2)\\top}$\n",
        "- ...\n",
        "- $\\omega_{2T} = x^{(T)} x^{(2)\\top} = x^{(T)} q^{(2)\\top}$\n",
        "\n",
        "$\\omega_{21}$에 있는 아래첨자 \"21\"은 입력 시퀀스 원소 1에 대해 입력 시퀀스 원소 2를 쿼리로 사용한다는 의미입니다."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "35e55f7a-f2d0-4f24-858b-228e4fe88fb3",
      "metadata": {
        "id": "35e55f7a-f2d0-4f24-858b-228e4fe88fb3"
      },
      "source": [
        "3차원 벡터로 임베딩한 아주 간단한 입력 시퀀스가 있다고 가정해 보도록 하겠습니다.  \n",
        "각 행은 단어를 나타내며, 각 열은 임베딩 차원을 나타냅니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "22b9556a-aaf8-4ab4-a5b4-973372b0b2c3",
      "metadata": {
        "id": "22b9556a-aaf8-4ab4-a5b4-973372b0b2c3"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "inputs = torch.tensor(\n",
        "  [[0.43, 0.15, 0.89], # Your     (x^1)\n",
        "   [0.55, 0.87, 0.66], # journey  (x^2)\n",
        "   [0.57, 0.85, 0.64], # starts   (x^3)\n",
        "   [0.22, 0.58, 0.33], # with     (x^4)\n",
        "   [0.77, 0.25, 0.10], # one      (x^5)\n",
        "   [0.05, 0.80, 0.55]] # step     (x^6)\n",
        ")\n",
        "\n",
        "# 3차원 임베딩 벡터라고 생각"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "299baef3-b1a8-49ba-bad4-f62c8a416d83",
      "metadata": {
        "id": "299baef3-b1a8-49ba-bad4-f62c8a416d83"
      },
      "source": [
        "두 번째 입력 원소 $x^{(2)}$를 쿼리로 사용해 context vector $z^{(2)}$를 계산해 보도록 하겠습니다.  \n",
        "$x^{(2)}$와 다른 모든 입력 원소 사이의 dot product를 통해 attention score $\\omega$를 계산합니다.\n",
        "\n",
        "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch03_compressed/08.webp\" width=\"1000px\">"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c5e36512",
      "metadata": {
        "id": "c5e36512"
      },
      "source": [
        "내적은 두 벡터 사이의 모든 원소를 곱하고 모두 합하여 구합니다.  \n",
        "$x^{(2)}$와 $x^{(0)}$ 간의 dot product를 구해보겠습니다.\n",
        "\n",
        "<img src=\"https://github.com/dusdnKR/SWCON425/blob/main/lec27_notebook_for_students/image/lec27_01.png?raw=1\" width=\"500px\">"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9842f39b-1654-410e-88bf-d1b899bf0241",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9842f39b-1654-410e-88bf-d1b899bf0241",
        "outputId": "31c68173-e157-440e-a178-ecebec5e0f3c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(0.9544)\n"
          ]
        }
      ],
      "source": [
        "query = ### your code\n",
        "input = ### your code\n",
        "res = 0.\n",
        "\n",
        "for idx, element in enumerate(input):\n",
        "    res += ### your code\n",
        "\n",
        "print(res)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "59fc40bb",
      "metadata": {
        "id": "59fc40bb"
      },
      "source": [
        "`torch`의 `dot()` 함수를 이용하여 쉽게 계산할 수 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9c391448",
      "metadata": {
        "id": "9c391448",
        "outputId": "e15e3bc6-93a7-4686-a742-f25443c77384"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(0.9544)\n"
          ]
        }
      ],
      "source": [
        "print(torch.dot(input, query))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "45c30865",
      "metadata": {
        "id": "45c30865"
      },
      "source": [
        "$x^{(2)}$와 다른 모든 벡터 간의 dot product를 수행하여 attention score을 구해보도록 하겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6fb5b2f8-dd2c-4a6d-94ef-a0e9ad163951",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6fb5b2f8-dd2c-4a6d-94ef-a0e9ad163951",
        "outputId": "0f2ddc92-0973-4913-c56b-861bb602a2b3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865])\n"
          ]
        }
      ],
      "source": [
        "query = ### your code\n",
        "\n",
        "attn_scores_2 = torch.empty(inputs.shape[0])\n",
        "for i, x_i in enumerate(inputs):\n",
        "    attn_scores_2[i] = ### your code\n",
        "\n",
        "print(attn_scores_2)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7d444d76-e19e-4e9a-a268-f315d966609b",
      "metadata": {
        "id": "7d444d76-e19e-4e9a-a268-f315d966609b"
      },
      "source": [
        "다음으로, attention score ($\\omega$)를 합이 1이 되도록 정규화하여 <b>attention weight</b>를 구해줍니다.  \n",
        "정규화되지 않은 어텐션 점수를 합이 1이 되도록 정규화하는 간단한 방법은 모든 어텐션 점수의 합으로 나누는 것입니다.\n",
        "\n",
        "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch03_compressed/09.webp\" width=\"1000px\">"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e3ccc99c-33ce-4f11-b7f2-353cf1cbdaba",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e3ccc99c-33ce-4f11-b7f2-353cf1cbdaba",
        "outputId": "85d600ce-f5a9-4d1f-9561-c26e30caf533"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "어텐션 가중치: tensor([0.1455, 0.2278, 0.2249, 0.1285, 0.1077, 0.1656])\n",
            "합: tensor(1.0000)\n"
          ]
        }
      ],
      "source": [
        "attn_weights_2_tmp = ### your code\n",
        "\n",
        "print(\"어텐션 가중치:\", attn_weights_2_tmp)\n",
        "print(\"합:\", attn_weights_2_tmp.sum())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "75dc0a57-f53e-41bf-8793-daa77a819431",
      "metadata": {
        "id": "75dc0a57-f53e-41bf-8793-daa77a819431"
      },
      "source": [
        "실제로는 소프트맥스 함수로 정규화를 수행합니다. 극한 값을 잘 다루고 훈련 과정에 유용한 그레이디언트 속성을 제공하기 때문입니다.  \n",
        "단순한 소프트맥스 함수를 이용하는 정규화 방식도 구현해보도록 하겠습니다.\n",
        "\n",
        "<img src=\"https://github.com/dusdnKR/SWCON425/blob/main/lec27_notebook_for_students/image/lec27_02.png?raw=1\" width=\"800px\">"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "07b2e58d-a6ed-49f0-a1cd-2463e8d53a20",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "07b2e58d-a6ed-49f0-a1cd-2463e8d53a20",
        "outputId": "f568e251-4982-473b-fac9-7ce7acd2e486"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "어텐션 가중치: tensor([0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581])\n",
            "합: tensor(1.)\n"
          ]
        }
      ],
      "source": [
        "def softmax_naive(x):\n",
        "    return ### your code\n",
        "\n",
        "attn_weights_2_naive = softmax_naive(attn_scores_2)\n",
        "\n",
        "print(\"어텐션 가중치:\", attn_weights_2_naive)\n",
        "print(\"합:\", attn_weights_2_naive.sum())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f0a1cbbb-4744-41cb-8910-f5c1355555fb",
      "metadata": {
        "id": "f0a1cbbb-4744-41cb-8910-f5c1355555fb"
      },
      "source": [
        "이론적으로는 위와 같이 계산이 가능하지만, 최적화가 되지 않은 상태라 입력이 너무 크거나 작으면 오버플로우나 언더플로우가 발생할 수 있습니다.  \n",
        "실제로는 최적화된 `torch` 라이브러리의 `softmax()` 함수를 사용하는 것이 좋습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2d99cac4-45ea-46b3-b3c1-e000ad16e158",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2d99cac4-45ea-46b3-b3c1-e000ad16e158",
        "outputId": "71a5115b-532e-4cd4-b047-439f44478276"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "어텐션 가중치: tensor([0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581])\n",
            "합: tensor(1.)\n"
          ]
        }
      ],
      "source": [
        "attn_weights_2 = ### your code\n",
        "\n",
        "print(\"어텐션 가중치:\", attn_weights_2)\n",
        "print(\"합:\", attn_weights_2.sum())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e43e36c7-90b2-427f-94f6-bb9d31b2ab3f",
      "metadata": {
        "id": "e43e36c7-90b2-427f-94f6-bb9d31b2ab3f"
      },
      "source": [
        "다음으로, 각각의 임베딩 토큰 $x^{(i)}$에 계산된 attention weignt를 곱하고 모두 합해 context vector $z^{(2)}$를 구합니다.  \n",
        "쿼리 토큰 $x^{(2)}$에 대한 attention weignt 값이 높을수록 context vector에 더 큰 영향을 미치게 됩니다.\n",
        "\n",
        "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch03_compressed/10.webp\" width=\"1000px\">"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8fcb96f0-14e5-4973-a50e-79ea7c6af99f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8fcb96f0-14e5-4973-a50e-79ea7c6af99f",
        "outputId": "962afa09-8283-404c-819d-bad00a5d89ec"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([0.4419, 0.6515, 0.5683])\n"
          ]
        }
      ],
      "source": [
        "query = inputs[1]\n",
        "\n",
        "context_vec_2 = torch.zeros(query.shape)\n",
        "for i,x_i in enumerate(inputs):\n",
        "    context_vec_2 += ### your code\n",
        "\n",
        "print(context_vec_2)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5a454262-40eb-430e-9ca4-e43fb8d6cd89",
      "metadata": {
        "id": "5a454262-40eb-430e-9ca4-e43fb8d6cd89"
      },
      "source": [
        "### 3.3.2 모든 입력 토큰에 대해 어텐션 가중치 계산하기"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "11c0fb55-394f-42f4-ba07-d01ae5c98ab4",
      "metadata": {
        "id": "11c0fb55-394f-42f4-ba07-d01ae5c98ab4"
      },
      "source": [
        "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch03_compressed/11.webp\" width=\"1000px\">"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b789b990-fb51-4beb-9212-bf58876b5983",
      "metadata": {
        "id": "b789b990-fb51-4beb-9212-bf58876b5983"
      },
      "source": [
        "#### 모든 입력 토큰에 일반화하기\n",
        "\n",
        "방금까지 self attention에 대해 알아보기 위하여 $x^{(2)}$에 대한  \n",
        "<b>(1) attention score를 계산하고,  \n",
        "(2) 합이 1이 되도록 정규화하여 attention weignt를 구한 뒤에,  \n",
        "(3) 이 결과를 가중합하여 context vector를 만들었습니다.</b>\n",
        "\n",
        "이 계산을 일반화하여 모든 attention weight와 context vector를 계산하겠습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d9bffe4b-56fe-4c37-9762-24bd924b7d3c",
      "metadata": {
        "id": "d9bffe4b-56fe-4c37-9762-24bd924b7d3c"
      },
      "source": [
        "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch03_compressed/12.webp\" width=\"1000px\">"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aa652506-f2c8-473c-a905-85c389c842cc",
      "metadata": {
        "id": "aa652506-f2c8-473c-a905-85c389c842cc"
      },
      "source": [
        "우선, 모든 원소 쌍에 대한 attention score을 계산해보겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "04004be8-07a1-468b-ab33-32e16a551b45",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "04004be8-07a1-468b-ab33-32e16a551b45",
        "outputId": "b3c50d24-0016-477c-c869-17090cafd538"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[0.9995, 0.9544, 0.9422, 0.4753, 0.4576, 0.6310],\n",
            "        [0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865],\n",
            "        [0.9422, 1.4754, 1.4570, 0.8296, 0.7154, 1.0605],\n",
            "        [0.4753, 0.8434, 0.8296, 0.4937, 0.3474, 0.6565],\n",
            "        [0.4576, 0.7070, 0.7154, 0.3474, 0.6654, 0.2935],\n",
            "        [0.6310, 1.0865, 1.0605, 0.6565, 0.2935, 0.9450]])\n"
          ]
        }
      ],
      "source": [
        "attn_scores = ### your code\n",
        "\n",
        "for i, x_i in enumerate(inputs):\n",
        "    for j, x_j in enumerate(inputs):\n",
        "        attn_scores[i, j] = ### your code\n",
        "\n",
        "print(attn_scores)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1539187f-1ece-47b7-bc9b-65a97115f1d4",
      "metadata": {
        "id": "1539187f-1ece-47b7-bc9b-65a97115f1d4"
      },
      "source": [
        "행렬 곱을 이용하면 동일한 행렬을 더 효율적으로 계산할 수 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2cea69d0-9a47-45da-8d5a-47ceef2df673",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2cea69d0-9a47-45da-8d5a-47ceef2df673",
        "outputId": "5bb94f4f-2a10-451e-f8b3-e41929176232"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[0.9995, 0.9544, 0.9422, 0.4753, 0.4576, 0.6310],\n",
            "        [0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865],\n",
            "        [0.9422, 1.4754, 1.4570, 0.8296, 0.7154, 1.0605],\n",
            "        [0.4753, 0.8434, 0.8296, 0.4937, 0.3474, 0.6565],\n",
            "        [0.4576, 0.7070, 0.7154, 0.3474, 0.6654, 0.2935],\n",
            "        [0.6310, 1.0865, 1.0605, 0.6565, 0.2935, 0.9450]])\n"
          ]
        }
      ],
      "source": [
        "attn_scores = ### your code\n",
        "print(attn_scores)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "02c4bac4-acfd-427f-9b11-c436ac71748d",
      "metadata": {
        "id": "02c4bac4-acfd-427f-9b11-c436ac71748d"
      },
      "source": [
        "이제, 각 행의 값이 1이 되도록 정규화하여 attention weight를 계산합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fa4ef062-de81-47ee-8415-bfe1708c81b8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fa4ef062-de81-47ee-8415-bfe1708c81b8",
        "outputId": "40fb1bed-e2db-407f-c0fa-35e266bcea0d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[0.2098, 0.2006, 0.1981, 0.1242, 0.1220, 0.1452],\n",
            "        [0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581],\n",
            "        [0.1390, 0.2369, 0.2326, 0.1242, 0.1108, 0.1565],\n",
            "        [0.1435, 0.2074, 0.2046, 0.1462, 0.1263, 0.1720],\n",
            "        [0.1526, 0.1958, 0.1975, 0.1367, 0.1879, 0.1295],\n",
            "        [0.1385, 0.2184, 0.2128, 0.1420, 0.0988, 0.1896]])\n"
          ]
        }
      ],
      "source": [
        "attn_weights = ### your code\n",
        "print(attn_weights)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3fa6d02b-7f15-4eb4-83a7-0b8a819e7a0c",
      "metadata": {
        "id": "3fa6d02b-7f15-4eb4-83a7-0b8a819e7a0c"
      },
      "source": [
        "각 행의 합이 정말로 1이 되는지 확인해 보겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "112b492c-fb6f-4e6d-8df5-518ae83363d5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "112b492c-fb6f-4e6d-8df5-518ae83363d5",
        "outputId": "58fff4f8-8bf9-4de5-cd05-227498842c8c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000])\n"
          ]
        }
      ],
      "source": [
        "print(attn_weights.sum(dim=-1))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "138b0b5c-d813-44c7-b373-fde9540ddfd1",
      "metadata": {
        "id": "138b0b5c-d813-44c7-b373-fde9540ddfd1"
      },
      "source": [
        "계산된 attention weight를 입력 임베딩에 곱하여 모든 context vector를 계산합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ba8eafcf-f7f7-4989-b8dc-61b50c4f81dc",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ba8eafcf-f7f7-4989-b8dc-61b50c4f81dc",
        "outputId": "0351ec4e-4d7d-4936-f68d-126b65eabc4e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[0.4421, 0.5931, 0.5790],\n",
            "        [0.4419, 0.6515, 0.5683],\n",
            "        [0.4431, 0.6496, 0.5671],\n",
            "        [0.4304, 0.6298, 0.5510],\n",
            "        [0.4671, 0.5910, 0.5266],\n",
            "        [0.4177, 0.6503, 0.5645]])\n"
          ]
        }
      ],
      "source": [
        "all_context_vecs = ### your code\n",
        "print(all_context_vecs)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "25b245b8-7732-4fab-aa1c-e3d333195605",
      "metadata": {
        "id": "25b245b8-7732-4fab-aa1c-e3d333195605"
      },
      "source": [
        "이전에 계산했던 문맥 벡터 $z^{(2)}$와도 같은 결과인 것을 확인할 수 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2570eb7d-aee1-457a-a61e-7544478219fa",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2570eb7d-aee1-457a-a61e-7544478219fa",
        "outputId": "13b99fa5-7dd9-43ec-9be8-9946c3ecebad"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "이전에 계산한 두 번째 문맥 벡터: tensor([0.4419, 0.6515, 0.5683])\n",
            "방금 계산한 두 번째 문맥 벡터: tensor([0.4419, 0.6515, 0.5683])\n"
          ]
        }
      ],
      "source": [
        "print(\"이전에 계산한 두 번째 문맥 벡터:\", context_vec_2)\n",
        "print(\"방금 계산한 두 번째 문맥 벡터:\", all_context_vecs[1])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a303b6fb-9f7e-42bb-9fdb-2adabf0a6525",
      "metadata": {
        "id": "a303b6fb-9f7e-42bb-9fdb-2adabf0a6525"
      },
      "source": [
        "## 3.4 훈련 가능한 가중치를 가진 셀프 어텐션 구현하기"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2b90a77e-d746-4704-9354-1ddad86e6298",
      "metadata": {
        "id": "2b90a77e-d746-4704-9354-1ddad86e6298"
      },
      "source": [
        "### 3.4.1 단계별로 어텐션 가중치 계산하기"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "46e95a46-1f67-4b71-9e84-8e2db84ab036",
      "metadata": {
        "id": "46e95a46-1f67-4b71-9e84-8e2db84ab036"
      },
      "source": [
        "원본 트랜스포머, GPT 모델 그외 다른 LLM에서 사용하는 셀프 어텐션 메커니즘을 구현해 보겠습니다.  \n",
        "방금 구현한 기본적인 어텐션 메커니즘과의 차이는 <b>가중치 행렬 $W_q$, $W_k$, $W_v$</b>가 추가되었다는 것입니다.  \n",
        "이 가중치 행렬은 학습 과정에서 업데이트되며 좋은 context vector를 만드는 방법을 학습하게 됩니다.\n",
        "\n",
        "<img src=\"https://github.com/dusdnKR/SWCON425/blob/main/lec27_notebook_for_students/image/lec27_03.png?raw=1\" width=\"1000px\">\n",
        "\n",
        "이전 과정과의 차이는 그림과 같습니다.  \n",
        "추가된 가중치 행렬 $W_q$, $W_k$, $W_v$는 임베딩된 입력 토큰 $x^{(i)}$과 곱해져 각각 query, key, value 벡터로 사용됩니다.\n",
        "  - $q^{(i)} = x^{(i)} \\,W_q$\n",
        "  - $k^{(i)} = x^{(i)} \\,W_k$\n",
        "  - $v^{(i)} = x^{(i)} \\,W_v$\n",
        "\n",
        "query, key, value라는 이름은 DB 용어에서 파생되었으며, 이름과 유사하게 각각 연산 기준, 비교 대상, 실제 벡터의 값의 역할을 수행합니다.  \n",
        "그림을 통해 기존 방식과 비교했을 때 어떤 위치에서 사용되는지 알 수 있습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9f334313-5fd0-477b-8728-04080a427049",
      "metadata": {
        "id": "9f334313-5fd0-477b-8728-04080a427049"
      },
      "source": [
        "입력 $x$와 쿼리 벡터 $q$의 임베딩 차원은 같은 수도 있고 다를 수도 있습니다. 모델 설계와 특정 구현에 따라 결정됩니다.  \n",
        "GPT 모델에서는 입력과 출력 차원이 일반적으로 같습니다. 하지만 계산 과정을 설명하기 쉽도록 입력과 출력 차원을 다르게 하겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8250fdc6-6cd6-4c5b-b9c0-8c643aadb7db",
      "metadata": {
        "id": "8250fdc6-6cd6-4c5b-b9c0-8c643aadb7db"
      },
      "outputs": [],
      "source": [
        "x_2 = inputs[1]\n",
        "d_in = inputs.shape[1] # 입력 임베딩 크기, d=3\n",
        "d_out = 2 # 출력 임베딩 크기, d=2"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f528cfb3-e226-47dd-b363-cc2caaeba4bf",
      "metadata": {
        "id": "f528cfb3-e226-47dd-b363-cc2caaeba4bf"
      },
      "source": [
        "아래에서 세 개의 가중치 행렬을 초기화합니다.  \n",
        "추후 `requires_grad=True`로 지정해서 훈련 과정 중에 이 행렬이 업데이트되도록 수정해야 합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bfd7259a-f26c-4cea-b8fc-282b5cae1e00",
      "metadata": {
        "id": "bfd7259a-f26c-4cea-b8fc-282b5cae1e00"
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(123)\n",
        "\n",
        "W_query = ### your code\n",
        "W_key   = ### your code\n",
        "W_value = ### your code"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "abfd0b50-7701-4adb-821c-e5433622d9c4",
      "metadata": {
        "id": "abfd0b50-7701-4adb-821c-e5433622d9c4"
      },
      "source": [
        "각각의 가중치 행렬을 이용해 query, key, value를 계산해 보겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "73cedd62-01e1-4196-a575-baecc6095601",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "73cedd62-01e1-4196-a575-baecc6095601",
        "outputId": "4523775d-bd9b-4d14-ef35-bfd66ec6f00d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([0.4306, 1.4551])\n"
          ]
        }
      ],
      "source": [
        "query_2 = ### your code\n",
        "key_2 = ### your code\n",
        "value_2 = ### your code\n",
        "\n",
        "print(query_2)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9be308b3-aca3-421b-b182-19c3a03b71c7",
      "metadata": {
        "id": "9be308b3-aca3-421b-b182-19c3a03b71c7"
      },
      "source": [
        "이제 모든 입력 토큰에 대한 query, key, value를 구해보겠습니다.  \n",
        "행렬의 크기를 확인해보면, 6개의 입력 토큰에 대한 3차원의 context vector가 2차원 임베딩 공간에 투영된 것을 확인할 수 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8c1c3949-fc08-4d19-a41e-1c235b4e631b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8c1c3949-fc08-4d19-a41e-1c235b4e631b",
        "outputId": "53c36da0-4304-4f99-f592-6198c5195c53"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "inputs.shape: torch.Size([6, 3])\n",
            "querys.shape: torch.Size([6, 2])\n",
            "keys.shape: torch.Size([6, 2])\n",
            "values.shape: torch.Size([6, 2])\n"
          ]
        }
      ],
      "source": [
        "querys = ### your code\n",
        "keys = ### your code\n",
        "values = ### your code\n",
        "\n",
        "print(\"inputs.shape:\", inputs.shape)\n",
        "print(\"querys.shape:\", querys.shape)\n",
        "print(\"keys.shape:\", keys.shape)\n",
        "print(\"values.shape:\", values.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bac5dfd6-ade8-4e7b-b0c1-bed40aa24481",
      "metadata": {
        "id": "bac5dfd6-ade8-4e7b-b0c1-bed40aa24481"
      },
      "source": [
        "다음으로, 기준이 되는 query와 각각의 key 벡터 사이의 dot product를 수행해 attention score을 계산합니다."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8ed0a2b7-5c50-4ede-90cf-7ad74412b3aa",
      "metadata": {
        "id": "8ed0a2b7-5c50-4ede-90cf-7ad74412b3aa"
      },
      "source": [
        "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch03_compressed/15.webp\" width=\"1000px\">"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "64cbc253-a182-4490-a765-246979ea0a28",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "64cbc253-a182-4490-a765-246979ea0a28",
        "outputId": "cdd85f6c-c00d-4118-aff4-719febf9dcb7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor(1.8524)\n"
          ]
        }
      ],
      "source": [
        "keys_2 = keys[1]\n",
        "attn_score_22 = ### your code\n",
        "print(attn_score_22)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9e9d15c0-c24e-4e6f-a160-6349b418f935",
      "metadata": {
        "id": "9e9d15c0-c24e-4e6f-a160-6349b418f935"
      },
      "source": [
        "이제 6개의 여섯 개의 입력 토큰 모두에 대해서도 구해보겠습니다.  \n",
        "query 벡터에 대한 여섯 개의 attention score가 만들어집니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b14e44b5-d170-40f9-8847-8990804af26d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b14e44b5-d170-40f9-8847-8990804af26d",
        "outputId": "645d61db-a200-4f6b-e26f-e7230ffe86b9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([1.2705, 1.8524, 1.8111, 1.0795, 0.5577, 1.5440])\n"
          ]
        }
      ],
      "source": [
        "attn_scores_2 = ### your code\n",
        "print(attn_scores_2)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8622cf39-155f-4eb5-a0c0-82a03ce9b999",
      "metadata": {
        "id": "8622cf39-155f-4eb5-a0c0-82a03ce9b999"
      },
      "source": [
        "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch03_compressed/16.webp\" width=\"800px\">"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e1609edb-f089-461a-8de2-c20c1bb29836",
      "metadata": {
        "id": "e1609edb-f089-461a-8de2-c20c1bb29836"
      },
      "source": [
        "이제 소프트맥스 함수를 사용해 attention score를 정규화하여 attention weight를 구해보겠습니다.  \n",
        "이전과 차이점은 소프트맥스 적용 전에 임베딩 차원의 제곱근 $\\sqrt{d_k}$로 나누어 어텐션 점수의 스케일을 조정하는 것입니다.\n",
        "\n",
        "<img src=\"https://github.com/dusdnKR/SWCON425/blob/main/lec27_notebook_for_students/image/lec27_02.png?raw=1\" width=\"500px\">\n",
        "\n",
        "입력 값이 너무 커지거나 작아질 경우 기울기가 과도하게 작아지는 Gradient Vanishing 문제가 발생하여 학습에 악영향을 끼칠 수 있습니다.  \n",
        "attention score을 $\\sqrt{d_k}$로 나누는 것은 이러한 문제를 예방하기 위함입니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "146f5587-c845-4e30-9894-c7ed3a248153",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "146f5587-c845-4e30-9894-c7ed3a248153",
        "outputId": "3b05e2c3-4478-4c32-936d-bbc7506881c0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([0.1500, 0.2264, 0.2199, 0.1311, 0.0906, 0.1820])\n"
          ]
        }
      ],
      "source": [
        "d_k = keys.shape[1]\n",
        "attn_weights_2 = ### your code\n",
        "print(attn_weights_2)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b8f61a28-b103-434a-aee1-ae7cbd821126",
      "metadata": {
        "id": "b8f61a28-b103-434a-aee1-ae7cbd821126"
      },
      "source": [
        "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch03_compressed/17.webp\" width=\"1000px\">"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1890e3f9-db86-4ab8-9f3b-53113504a61f",
      "metadata": {
        "id": "1890e3f9-db86-4ab8-9f3b-53113504a61f"
      },
      "source": [
        "이제 두 번째 입력 query 벡터에 대한 context vector를 계산합니다.  \n",
        "방금 구한 attention weight를 value 벡터에 곱하여 구합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e138f033-fa7e-4e3a-8764-b53a96b26397",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e138f033-fa7e-4e3a-8764-b53a96b26397",
        "outputId": "6db25563-7a3b-40c6-a7a7-3749d993f586"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([0.3061, 0.8210])\n"
          ]
        }
      ],
      "source": [
        "context_vec_2 = ### your code\n",
        "print(context_vec_2)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9d7b2907-e448-473e-b46c-77735a7281d8",
      "metadata": {
        "id": "9d7b2907-e448-473e-b46c-77735a7281d8"
      },
      "source": [
        "### 3.4.2 셀프 어텐션 파이썬 클래스 구현하기"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "04313410-3155-4d90-a7a3-2f3386e73677",
      "metadata": {
        "id": "04313410-3155-4d90-a7a3-2f3386e73677"
      },
      "source": [
        "이제 이 모든 과정을 한 번에 수행하는 `SelfAttention_v1`을 구현해 보겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "51590326-cdbe-4e62-93b1-17df71c11ee4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "51590326-cdbe-4e62-93b1-17df71c11ee4",
        "outputId": "8efbc837-0ec9-4d9b-fccd-f8c1c5308afc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[0.2996, 0.8053],\n",
            "        [0.3061, 0.8210],\n",
            "        [0.3058, 0.8203],\n",
            "        [0.2948, 0.7939],\n",
            "        [0.2927, 0.7891],\n",
            "        [0.2990, 0.8040]], grad_fn=<MmBackward0>)\n"
          ]
        }
      ],
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class SelfAttention_v1(nn.Module):\n",
        "\n",
        "    def __init__(self, d_in, d_out):\n",
        "        super().__init__()\n",
        "        self.W_query = ### your code\n",
        "        self.W_key   = ### your code\n",
        "        self.W_value = ### your code\n",
        "\n",
        "    def forward(self, x):\n",
        "        keys = ### your code\n",
        "        queries = ### your code\n",
        "        values = ### your code\n",
        "\n",
        "        attn_scores = ### your code\n",
        "        attn_weights = ### your code\n",
        "\n",
        "        context_vec = ### your code\n",
        "        return context_vec\n",
        "\n",
        "torch.manual_seed(123)\n",
        "sa_v1 = SelfAttention_v1(d_in, d_out)\n",
        "print(sa_v1(inputs))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7ee1a024-84a5-425a-9567-54ab4e4ed445",
      "metadata": {
        "id": "7ee1a024-84a5-425a-9567-54ab4e4ed445"
      },
      "source": [
        "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch03_compressed/18.webp\" width=\"1000px\">"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "048e0c16-d911-4ec8-b0bc-45ceec75c081",
      "metadata": {
        "id": "048e0c16-d911-4ec8-b0bc-45ceec75c081"
      },
      "source": [
        "이 모든 과정은 파이토치의 `Linear` layer을 통해 더 간단하고 효율적으로 구현할 수 있습니다.  \n",
        "방금 구현에 이용한 `nn.Parameter(torch.rand(...)`와 달리 `nn.Linear`는 가중치 초기화를 제공하여 더 안정적으로 모델을 훈련할 수 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "73f411e3-e231-464a-89fe-0a9035e5f839",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "73f411e3-e231-464a-89fe-0a9035e5f839",
        "outputId": "fc3b85e9-270e-4364-c97a-71f6acce3cb9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[-0.0739,  0.0713],\n",
            "        [-0.0748,  0.0703],\n",
            "        [-0.0749,  0.0702],\n",
            "        [-0.0760,  0.0685],\n",
            "        [-0.0763,  0.0679],\n",
            "        [-0.0754,  0.0693]], grad_fn=<MmBackward0>)\n"
          ]
        }
      ],
      "source": [
        "class SelfAttention_v2(nn.Module):\n",
        "\n",
        "    def __init__(self, d_in, d_out, qkv_bias=False):\n",
        "        super().__init__()\n",
        "        self.W_query = ### your code\n",
        "        self.W_key   = ### your code\n",
        "        self.W_value = ### your code\n",
        "\n",
        "    def forward(self, x):\n",
        "        keys = self.W_key(x)\n",
        "        queries = self.W_query(x)\n",
        "        values = self.W_value(x)\n",
        "\n",
        "        attn_scores = queries @ keys.T\n",
        "        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
        "\n",
        "        context_vec = attn_weights @ values\n",
        "        return context_vec\n",
        "\n",
        "torch.manual_seed(789)\n",
        "sa_v2 = SelfAttention_v2(d_in, d_out)\n",
        "print(sa_v2(inputs))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "915cd8a5-a895-42c9-8b8e-06b5ae19ffce",
      "metadata": {
        "id": "915cd8a5-a895-42c9-8b8e-06b5ae19ffce"
      },
      "source": [
        "가중치 행렬 초깃값이 다르기 때문에 `SelfAttention_v1`와 `SelfAttention_v2`의 출력이 다릅니다."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c5025b37-0f2c-4a67-a7cb-1286af7026ab",
      "metadata": {
        "id": "c5025b37-0f2c-4a67-a7cb-1286af7026ab"
      },
      "source": [
        "## 3.5 코잘 어텐션으로 미래의 단어를 감추기"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aef0a6b8-205a-45bf-9d26-8fd77a8a03c3",
      "metadata": {
        "id": "aef0a6b8-205a-45bf-9d26-8fd77a8a03c3"
      },
      "source": [
        "방금 구현한 셀프 어텐션 class는 행렬 연산을 통해 모든 연산을 빠르게 수행할 수 있도록 만들어졌습니다.  \n",
        "하지만, 위와 같은 구조로 학습하게 되면 모델은 아직 알지 못하는 뒤의 토큰까지 attention 연산에 활용하게 됩니다.\n",
        "\n",
        "<img src=\"https://github.com/dusdnKR/SWCON425/blob/main/lec27_notebook_for_students/image/lec27_04.png?raw=1\" width=\"1000px\">"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "71e91bb5-5aae-4f05-8a95-973b3f988a35",
      "metadata": {
        "id": "71e91bb5-5aae-4f05-8a95-973b3f988a35"
      },
      "source": [
        "코잘 어텐션(causal attention)은 주대각선 위의 attention weight를 마스킹합니다.  \n",
        "이를 통해 행렬 연산으로 모든 계산을 한 번에 수행하면서도 새로운 context vector가 미래 토큰을 참조할 수 없도록 합니다.\n",
        "\n",
        "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch03_compressed/19.webp\" width=\"1000px\">"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "82f405de-cd86-4e72-8f3c-9ea0354946ba",
      "metadata": {
        "id": "82f405de-cd86-4e72-8f3c-9ea0354946ba"
      },
      "source": [
        "### 3.5.1 코잘 어텐션 마스크 적용하기"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "014f28d0-8218-48e4-8b9c-bdc5ce489218",
      "metadata": {
        "id": "014f28d0-8218-48e4-8b9c-bdc5ce489218"
      },
      "source": [
        "코잘 어텐션을 실제로 구현해보도록 하겠습니다.\n",
        "\n",
        "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch03_compressed/20.webp\" width=\"1000px\">"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "38ce7b08",
      "metadata": {
        "id": "38ce7b08"
      },
      "source": [
        "방금 만든 `SelfAttention_v2` 객체의 쿼리와 키 가중치 행렬을 기반으로 구현해보도록 하겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1933940d-0fa5-4b17-a3ce-388e5314a1bb",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1933940d-0fa5-4b17-a3ce-388e5314a1bb",
        "outputId": "7634f68f-e0c6-48bd-cff1-b1fa4a336eea"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[0.1921, 0.1646, 0.1652, 0.1550, 0.1721, 0.1510],\n",
            "        [0.2041, 0.1659, 0.1662, 0.1496, 0.1665, 0.1477],\n",
            "        [0.2036, 0.1659, 0.1662, 0.1498, 0.1664, 0.1480],\n",
            "        [0.1869, 0.1667, 0.1668, 0.1571, 0.1661, 0.1564],\n",
            "        [0.1830, 0.1669, 0.1670, 0.1588, 0.1658, 0.1585],\n",
            "        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\n",
            "       grad_fn=<SoftmaxBackward0>)\n"
          ]
        }
      ],
      "source": [
        "queries = sa_v2.W_query(inputs)\n",
        "keys = sa_v2.W_key(inputs)\n",
        "attn_scores = queries @ keys.T\n",
        "\n",
        "attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
        "print(attn_weights)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "89020a96-b34d-41f8-9349-98c3e23fd5d6",
      "metadata": {
        "id": "89020a96-b34d-41f8-9349-98c3e23fd5d6"
      },
      "source": [
        "미래 시점의 attention weight를 마스킹하는 가장 간단한 방법은 파이토치 `tril` 함수를 이용하는 것입니다.  \n",
        "이를 통해 주대각선과 그 아래의 원소는 1, 주대각선 위의 원소는 0인 마스크를 만들 수 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "43f3d2e3-185b-4184-9f98-edde5e6df746",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "43f3d2e3-185b-4184-9f98-edde5e6df746",
        "outputId": "9edbd903-47f8-41de-d033-816303a526f1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[1., 0., 0., 0., 0., 0.],\n",
            "        [1., 1., 0., 0., 0., 0.],\n",
            "        [1., 1., 1., 0., 0., 0.],\n",
            "        [1., 1., 1., 1., 0., 0.],\n",
            "        [1., 1., 1., 1., 1., 0.],\n",
            "        [1., 1., 1., 1., 1., 1.]])\n"
          ]
        }
      ],
      "source": [
        "context_length = attn_scores.shape[0]\n",
        "mask_simple = ### your code\n",
        "print(mask_simple)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "efce2b08-3583-44da-b3fc-cabdd38761f6",
      "metadata": {
        "id": "efce2b08-3583-44da-b3fc-cabdd38761f6"
      },
      "source": [
        "그다음 attention weight와 마스크를 곱해서 주대각선 위의 값을 0으로 만듭니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9f531e2e-f4d2-4fea-a87f-4c132e48b9e7",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9f531e2e-f4d2-4fea-a87f-4c132e48b9e7",
        "outputId": "73ca4990-ad74-4428-ab28-4a1bc3d671e5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[0.1921, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.2041, 0.1659, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.2036, 0.1659, 0.1662, 0.0000, 0.0000, 0.0000],\n",
            "        [0.1869, 0.1667, 0.1668, 0.1571, 0.0000, 0.0000],\n",
            "        [0.1830, 0.1669, 0.1670, 0.1588, 0.1658, 0.0000],\n",
            "        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\n",
            "       grad_fn=<MulBackward0>)\n"
          ]
        }
      ],
      "source": [
        "masked_simple = ### your code\n",
        "print(masked_simple)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3eb35787-cf12-4024-b66d-e7215e175500",
      "metadata": {
        "id": "3eb35787-cf12-4024-b66d-e7215e175500"
      },
      "source": [
        "하지만 마스크를 소프트맥스 함수 이후에 적용하면 정규화 확률 분포가 망가지게 됩니다.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0011d369",
      "metadata": {
        "id": "0011d369",
        "outputId": "c4a0551c-214f-41c7-8bc4-4c79347640be"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([0.1921, 0.3700, 0.5357, 0.6775, 0.8415, 1.0000],\n",
            "       grad_fn=<SumBackward1>)\n"
          ]
        }
      ],
      "source": [
        "print(masked_simple.sum(dim=-1))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "94db92d7-c397-4e42-bd8a-6a2b3e237e0f",
      "metadata": {
        "id": "94db92d7-c397-4e42-bd8a-6a2b3e237e0f"
      },
      "source": [
        "각 행의 합이 1이 되도록 attention weight를 다시 정규화할 수 있습니다.  \n",
        "마스킹된 각 행의 합을 구해 나누어 줍니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6d392083-fd81-4f70-9bdf-8db985e673d6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6d392083-fd81-4f70-9bdf-8db985e673d6",
        "outputId": "51459318-27e9-4dea-86ac-c43bd79dbf21"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.5517, 0.4483, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.3800, 0.3097, 0.3103, 0.0000, 0.0000, 0.0000],\n",
            "        [0.2758, 0.2460, 0.2462, 0.2319, 0.0000, 0.0000],\n",
            "        [0.2175, 0.1983, 0.1984, 0.1888, 0.1971, 0.0000],\n",
            "        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\n",
            "       grad_fn=<DivBackward0>)\n",
            "tensor([1., 1., 1., 1., 1., 1.], grad_fn=<SumBackward1>)\n"
          ]
        }
      ],
      "source": [
        "masked_simple_norm = ### your code\n",
        "print(masked_simple_norm)\n",
        "print(masked_simple_norm.sum(dim=-1))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "512e7cf4-dc0e-4cec-948e-c7a3c4eb6877",
      "metadata": {
        "id": "512e7cf4-dc0e-4cec-948e-c7a3c4eb6877"
      },
      "source": [
        "이론적으로는 코잘 어텐션 메커니즘의 구현이 완료되었습니다.  \n",
        "하지만, 정규화를 두 번 수행하는 것보다 더 효율적인 방법이 필요하겠죠.\n",
        "\n",
        "<img src=\"https://github.com/dusdnKR/SWCON425/blob/main/lec27_notebook_for_students/image/lec27_02.png?raw=1\" width=\"500px\">\n",
        "\n",
        "소프트맥스 함수의 특성을 이용하면 정규화를 한 번만 수행하고도 동일한 결과를 얻을 수 있습니다.  \n",
        "소프트맥스는 음의 무한대가 입력으로 들어오면 출력이 0에 가까워집니다."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b038ab76",
      "metadata": {
        "id": "b038ab76"
      },
      "source": [
        "마스킹될 토큰의 attention score을 음의 무한대(`-inf`)로 대치시키겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a2be2f43-9cf0-44f6-8d8b-68ef2fb3cc39",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a2be2f43-9cf0-44f6-8d8b-68ef2fb3cc39",
        "outputId": "9f9cd2f8-acb9-4fc4-d219-fc8e832e3f17"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[0.2899,   -inf,   -inf,   -inf,   -inf,   -inf],\n",
            "        [0.4656, 0.1723,   -inf,   -inf,   -inf,   -inf],\n",
            "        [0.4594, 0.1703, 0.1731,   -inf,   -inf,   -inf],\n",
            "        [0.2642, 0.1024, 0.1036, 0.0186,   -inf,   -inf],\n",
            "        [0.2183, 0.0874, 0.0882, 0.0177, 0.0786,   -inf],\n",
            "        [0.3408, 0.1270, 0.1290, 0.0198, 0.1290, 0.0078]],\n",
            "       grad_fn=<MaskedFillBackward0>)\n"
          ]
        }
      ],
      "source": [
        "mask = torch.triu(torch.ones(context_length, context_length), diagonal=1)\n",
        "masked = ### your code\n",
        "print(masked)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "91d5f803-d735-4543-b9da-00ac10fb9c50",
      "metadata": {
        "id": "91d5f803-d735-4543-b9da-00ac10fb9c50"
      },
      "source": [
        "이 attention score에 소프트맥스 정규화를 적용하면 마스킹된 attention weight을 얻을 수 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b1cd6d7f-16f2-43c1-915e-0824f1a4bc52",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b1cd6d7f-16f2-43c1-915e-0824f1a4bc52",
        "outputId": "9f8e2cad-8f82-464b-be02-f713dbde508b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.5517, 0.4483, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.3800, 0.3097, 0.3103, 0.0000, 0.0000, 0.0000],\n",
            "        [0.2758, 0.2460, 0.2462, 0.2319, 0.0000, 0.0000],\n",
            "        [0.2175, 0.1983, 0.1984, 0.1888, 0.1971, 0.0000],\n",
            "        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],\n",
            "       grad_fn=<SoftmaxBackward0>)\n"
          ]
        }
      ],
      "source": [
        "attn_weights = torch.softmax(masked / keys.shape[-1]**0.5, dim=-1)\n",
        "print(attn_weights)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7636fc5f-6bc6-461e-ac6a-99ec8e3c0912",
      "metadata": {
        "id": "7636fc5f-6bc6-461e-ac6a-99ec8e3c0912"
      },
      "source": [
        "### 3.5.2 드롭아웃으로 어텐션 가중치에 추가적으로 마스킹하기"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ec3dc7ee-6539-4fab-804a-8f31a890c85a",
      "metadata": {
        "id": "ec3dc7ee-6539-4fab-804a-8f31a890c85a"
      },
      "source": [
        "모델 훈련에서는 overfitting을 막기 위해 종종 드롭아웃(dropout) 기법을 사용합니다.  \n",
        "이 방법은 랜덤한 값을 추가로 마스킹하여 모델이 훈련 데이터에 과도하게 의지하지 못하도록 막습니다.\n",
        "\n",
        "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch03_compressed/22.webp\" width=\"600px\">\n",
        "\n",
        "드롭아웃 마스크를 구현해보도록 하겠습니다.  \n",
        "본 실습에서는 마스크의 효과를 편하게 확인하게 위해 드롭아웃 비율 $p$를 50%으로 지정하였습니다.  \n",
        "하지만 추후 학습에는 0.1이나 0.2 정도의 비율을 사용하는 것이 좋습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0de578db-8289-41d6-b377-ef645751e33f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0de578db-8289-41d6-b377-ef645751e33f",
        "outputId": "8ac3a821-a3a7-4957-c1ee-b04cc5f38548"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[2., 2., 2., 2., 2., 2.],\n",
            "        [0., 2., 0., 0., 0., 0.],\n",
            "        [0., 0., 2., 0., 2., 0.],\n",
            "        [2., 2., 0., 0., 0., 2.],\n",
            "        [2., 0., 0., 0., 0., 2.],\n",
            "        [0., 2., 0., 0., 0., 0.]])\n"
          ]
        }
      ],
      "source": [
        "torch.manual_seed(123)\n",
        "dropout = ### your code\n",
        "example = torch.ones(6, 6)\n",
        "\n",
        "print(dropout(example))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cada5532",
      "metadata": {
        "id": "cada5532"
      },
      "source": [
        "드롭아웃 마스크 적용 결과를 확인해보면, 단순히 마스킹만 적용되는 것이 아니라 원본 값에도 변화가 생긴 것을 확인할 수 있습니다.  \n",
        "드롭아웃 마스크는 사라진 값을 보완하기 위해 남아있는 값에 $\\frac{1}{p}$를 곱해줍니다. 현재는 $p=0.5$이므로, $\\frac{1}{0.5}=2$배가 적용되었습니다.  \n",
        "실제 `attn_weights`에도 적용해보겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b16c5edb-942b-458c-8e95-25e4e355381e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b16c5edb-942b-458c-8e95-25e4e355381e",
        "outputId": "f5e02e35-7c86-4bc9-dadc-e76c182dd11b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[2.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.0000, 0.8966, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.0000, 0.0000, 0.6206, 0.0000, 0.0000, 0.0000],\n",
            "        [0.5517, 0.4921, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.4350, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.0000, 0.3327, 0.0000, 0.0000, 0.0000, 0.0000]],\n",
            "       grad_fn=<MulBackward0>)\n"
          ]
        }
      ],
      "source": [
        "torch.manual_seed(123)\n",
        "print(dropout(attn_weights))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cdc14639-5f0f-4840-aa9d-8eb36ea90fb7",
      "metadata": {
        "id": "cdc14639-5f0f-4840-aa9d-8eb36ea90fb7"
      },
      "source": [
        "### 3.5.3 코잘 어텐션 클래스 구현하기"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "09c41d29-1933-43dc-ada6-2dbb56287204",
      "metadata": {
        "id": "09c41d29-1933-43dc-ada6-2dbb56287204"
      },
      "source": [
        "이제 코잘 마스크와 드롭아웃 마스크가 적용된 셀프 어텐션 클래스를 만들어보겠습니다.  \n",
        "또한, 지난 실습에서 만든 데이터 로더의 배치 출력도 지원할 수 있도록 배치 처리를 고려해서 구현하겠습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e68afc35",
      "metadata": {
        "id": "e68afc35"
      },
      "source": [
        "batch size가 2이고, 각각의 batch가 6개의 토큰으로 구성된 3차원 임베딩 벡터가 있다고 가정하겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "977a5fa7-a9d5-4e2e-8a32-8e0331ccfe28",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "977a5fa7-a9d5-4e2e-8a32-8e0331ccfe28",
        "outputId": "52bbb773-08d1-48a0-b1d6-2ab75a7bc2d3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([2, 6, 3])\n"
          ]
        }
      ],
      "source": [
        "batch = torch.stack((inputs, inputs), dim=0)\n",
        "print(batch.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "56808944",
      "metadata": {
        "id": "56808944"
      },
      "source": [
        "코잘 마스크와 드롭아웃 마스크가 적용된 셀프 어텐션 클래스를 구현합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "60d8c2eb-2d8e-4d2c-99bc-9eef8cc53ca0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "60d8c2eb-2d8e-4d2c-99bc-9eef8cc53ca0",
        "outputId": "f7f5e19c-3c99-4e81-c6ad-d22fbfb46f38"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[[-0.4519,  0.2216],\n",
            "         [-0.5874,  0.0058],\n",
            "         [-0.6300, -0.0632],\n",
            "         [-0.5675, -0.0843],\n",
            "         [-0.5526, -0.0981],\n",
            "         [-0.5299, -0.1081]],\n",
            "\n",
            "        [[-0.4519,  0.2216],\n",
            "         [-0.5874,  0.0058],\n",
            "         [-0.6300, -0.0632],\n",
            "         [-0.5675, -0.0843],\n",
            "         [-0.5526, -0.0981],\n",
            "         [-0.5299, -0.1081]]], grad_fn=<UnsafeViewBackward0>)\n",
            "context_vecs.shape: torch.Size([2, 6, 2])\n"
          ]
        }
      ],
      "source": [
        "class CausalAttention(nn.Module):\n",
        "\n",
        "    def __init__(self, d_in, d_out, context_length,\n",
        "                 dropout, qkv_bias=False):\n",
        "        super().__init__()\n",
        "        # self.d_out = d_out\n",
        "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "        self.W_key   = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "        self.dropout = ### your code\n",
        "        ### your code\n",
        "\n",
        "    def forward(self, x):\n",
        "        ### your code\n",
        "        keys = self.W_key(x)\n",
        "        queries = self.W_query(x)\n",
        "        values = self.W_value(x)\n",
        "\n",
        "        attn_scores = queries @ keys.transpose(1, 2)\n",
        "        ### your code\n",
        "        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
        "        attn_weights = ### your code\n",
        "\n",
        "        context_vec = attn_weights @ values\n",
        "        return context_vec\n",
        "\n",
        "torch.manual_seed(123)\n",
        "\n",
        "context_length = batch.shape[1]\n",
        "ca = CausalAttention(d_in, d_out, context_length, 0.0)\n",
        "\n",
        "context_vecs = ca(batch)\n",
        "\n",
        "print(context_vecs)\n",
        "print(\"context_vecs.shape:\", context_vecs.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c8bef90f-cfd4-4289-b0e8-6a00dc9be44c",
      "metadata": {
        "id": "c8bef90f-cfd4-4289-b0e8-6a00dc9be44c"
      },
      "source": [
        "## 3.6 싱글 헤드 어텐션을 멀티 헤드 어텐션으로 확장하기"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "11697757-9198-4a1c-9cee-f450d8bbd3b9",
      "metadata": {
        "id": "11697757-9198-4a1c-9cee-f450d8bbd3b9"
      },
      "source": [
        "### 3.6.1 여러 개의 싱글 헤드 어텐션 층 쌓기"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "70766faf-cd53-41d9-8a17-f1b229756a5a",
      "metadata": {
        "id": "70766faf-cd53-41d9-8a17-f1b229756a5a"
      },
      "source": [
        "이러한 어텐션 모듈 각각을 헤드(head)라고 부릅니다.  \n",
        "방금 구현한 `CausalAttention` 클래스는 한 번의 어텐션 과정을 수행하는 <b>싱글 헤드 어텐션</b>과 같습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e8af30f6",
      "metadata": {
        "id": "e8af30f6"
      },
      "source": [
        "<img src=\"https://github.com/dusdnKR/SWCON425/blob/main/lec27_notebook_for_students/image/lec27_05.png?raw=1\" width=\"800px\">\n",
        "\n",
        "실제 트랜스포머 모델에서는 여러 개의 헤드를 가진 <b>멀티 헤드 어텐션</b>을 수행합니다.  \n",
        "각각의 헤드는 학습 가능한 고유한 가중치 행렬 $W_q$, $W_k$, $W_v$를 가지고 있습니다.  \n",
        "즉, 각각의 헤드는 서로 다른 방향으로 학습되어 다양한 관점에서 attention을 수행하게 됩니다.\n",
        "\n",
        "<img src=\"https://github.com/dusdnKR/SWCON425/blob/main/lec27_notebook_for_students/image/lec27_08.png?raw=1\" width=\"800px\">\n",
        "\n",
        "예를 들어, 어떤 방향으로 학습이 이루어졌는지에 따라 문장의 종류에 집중하거나, 특정 품사에 집중하거나, 품사 간의 관계에 집중할 수 있을 것입니다.\n",
        "\n",
        "<img src=\"https://github.com/dusdnKR/SWCON425/blob/main/lec27_notebook_for_students/image/lec27_06.png?raw=1\" width=\"800px\">\n",
        "\n",
        "위 이미지는 실제 어텐션 논문에 실린 attention 시각화 자료입니다.  \n",
        "\"its\"라는 단어를 기준으로 attention weight가 높은 단어를 확인해 보았을 때, 보라색 head는 \"its\"라는 대명사의 본래 의미에 집중하고 있으며, 갈색 head는 본래 의미를 나타내는 \"Law\"와 \"its\"가 꾸며주는 \"application\"에 동시에 집중하고 있습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a0805126",
      "metadata": {
        "id": "a0805126"
      },
      "source": [
        "<img src=\"https://github.com/dusdnKR/SWCON425/blob/main/lec27_notebook_for_students/image/lec27_07.png?raw=1\" width=\"800px\">\n",
        "\n",
        "멀티 헤드 어텐션의 결과는 단순히 각 헤드의 결과를 concat한 것과 같습니다.  \n",
        "그러므로, 이전에 구현한 `CausalAttention`을 여러 개 쌓아서 결과를 합치는 것만으로도 멀티 헤드 어텐션을 구현할 수 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b9a66e11-7105-4bb4-be84-041f1a1f3bd2",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b9a66e11-7105-4bb4-be84-041f1a1f3bd2",
        "outputId": "1f3df00b-77d7-4dff-ad4c-361eb96a04b1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[[-0.4519,  0.2216,  0.4772,  0.1063],\n",
            "         [-0.5874,  0.0058,  0.5891,  0.3257],\n",
            "         [-0.6300, -0.0632,  0.6202,  0.3860],\n",
            "         [-0.5675, -0.0843,  0.5478,  0.3589],\n",
            "         [-0.5526, -0.0981,  0.5321,  0.3428],\n",
            "         [-0.5299, -0.1081,  0.5077,  0.3493]],\n",
            "\n",
            "        [[-0.4519,  0.2216,  0.4772,  0.1063],\n",
            "         [-0.5874,  0.0058,  0.5891,  0.3257],\n",
            "         [-0.6300, -0.0632,  0.6202,  0.3860],\n",
            "         [-0.5675, -0.0843,  0.5478,  0.3589],\n",
            "         [-0.5526, -0.0981,  0.5321,  0.3428],\n",
            "         [-0.5299, -0.1081,  0.5077,  0.3493]]], grad_fn=<CatBackward0>)\n",
            "context_vecs.shape: torch.Size([2, 6, 4])\n"
          ]
        }
      ],
      "source": [
        "class MultiHeadAttentionWrapper(nn.Module):\n",
        "\n",
        "    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList(\n",
        "            [] ### your code\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return ### your code\n",
        "\n",
        "\n",
        "torch.manual_seed(123)\n",
        "\n",
        "context_length = batch.shape[1] # 토큰 개수\n",
        "d_in, d_out = 3, 2\n",
        "mha = MultiHeadAttentionWrapper(\n",
        "    d_in, d_out, context_length, 0.0, num_heads=2\n",
        ")\n",
        "\n",
        "context_vecs = mha(batch)\n",
        "\n",
        "print(context_vecs)\n",
        "print(\"context_vecs.shape:\", context_vecs.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "193d3d2b-2578-40ba-b791-ea2d49328e48",
      "metadata": {
        "id": "193d3d2b-2578-40ba-b791-ea2d49328e48"
      },
      "source": [
        "위 구현에서 임베딩 차원은 4입니다. 문맥 벡터는 물론 쿼리, 키, 값 벡터의 차원으로 `d_out=2`를 지정했기 때문입니다.  \n",
        "두 개의 어텐션 헤드가 있으므로 출력 임베딩 차원은 2*2=4가 됩니다."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6836b5da-ef82-4b4c-bda1-72a462e48d4e",
      "metadata": {
        "id": "6836b5da-ef82-4b4c-bda1-72a462e48d4e"
      },
      "source": [
        "### 3.6.2 가중치 분할로 멀티 헤드 어텐션 구현하기"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f4b48d0d-71ba-4fa0-b714-ca80cabcb6f7",
      "metadata": {
        "id": "f4b48d0d-71ba-4fa0-b714-ca80cabcb6f7"
      },
      "source": [
        "위에서 구현한 내용을 더 효율적으로 만들어 보겠습니다.\n",
        "\n",
        "<img src=\"https://sebastianraschka.com/images/LLMs-from-scratch-images/ch03_compressed/26.webp\" width=\"800px\">\n",
        "\n",
        "`CausalAttention` 여러 개를 각각 계산하여 concat하는 대신, 하나의 행렬이 여러 헤드 구역으로 나뉘어 있다고 생각하고 한 번의 행렬 연산으로 계산을 수행할 수 있도록 구현하겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "110b0188-6e9e-4e56-a988-10523c6c8538",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "110b0188-6e9e-4e56-a988-10523c6c8538",
        "outputId": "3237b108-3f74-479b-98dd-95b330ab81e3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "key.shape before reshape: torch.Size([2, 6, 4])\n",
            "b: 2 , num_tokens: 6 , d_out: 4\n",
            "\n",
            "keys.shape after reshape: torch.Size([2, 6, 2, 2])\n",
            "d_out: 4 = num_heads: 2 * head_dim: 2\n",
            "\n",
            "keys.shape after transpose: torch.Size([2, 2, 6, 2])\n",
            "b: 2 , num_heads: 2 , num_tokens: 6 , head_dim: 2\n",
            "\n",
            "attn_scores.shape: torch.Size([2, 2, 6, 6])\n",
            "[6,2] @ [6,2]T = [6,6]\n",
            "\n",
            "context_vec.shape after transpose: torch.Size([2, 6, 2, 2])\n",
            "b: 2 , num_tokens: 6 , num_heads: 2 , head_dim: 2\n",
            "\n",
            "context_vec.shape after reshape: torch.Size([2, 6, 4])\n",
            "b: 2 , num_tokens: 6 , d_out: 4\n",
            "\n",
            "tensor([[[ 0.1184,  0.3120, -0.0847, -0.5774],\n",
            "         [ 0.0178,  0.3221, -0.0763, -0.4225],\n",
            "         [-0.0147,  0.3259, -0.0734, -0.3721],\n",
            "         [-0.0116,  0.3138, -0.0708, -0.3624],\n",
            "         [-0.0117,  0.2973, -0.0698, -0.3543],\n",
            "         [-0.0132,  0.2990, -0.0689, -0.3490]],\n",
            "\n",
            "        [[ 0.1184,  0.3120, -0.0847, -0.5774],\n",
            "         [ 0.0178,  0.3221, -0.0763, -0.4225],\n",
            "         [-0.0147,  0.3259, -0.0734, -0.3721],\n",
            "         [-0.0116,  0.3138, -0.0708, -0.3624],\n",
            "         [-0.0117,  0.2973, -0.0698, -0.3543],\n",
            "         [-0.0132,  0.2990, -0.0689, -0.3490]]], grad_fn=<ViewBackward0>)\n",
            "context_vecs.shape: torch.Size([2, 6, 4])\n"
          ]
        }
      ],
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n",
        "        super().__init__()\n",
        "        assert (d_out % num_heads == 0), \\\n",
        "            \"d_out은 num_heads로 나누어 떨어져야 합니다\"\n",
        "\n",
        "        self.d_out = d_out\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = ### your code\n",
        "\n",
        "        self.W_query = ### your code\n",
        "        self.W_key = ### your code\n",
        "        self.W_value = ### your code\n",
        "        self.out_proj = nn.Linear(d_out, d_out) # Linear 층을 사용해 헤드의 출력을 결합합니다 (최근 연구에서는 이를 제거해도 성능에 영향을 미치지 않는다고 함)\n",
        "        self.dropout = ### your code\n",
        "        ### your code\n",
        "\n",
        "    def forward(self, x):\n",
        "        b, num_tokens, d_in = x.shape # b: 배치 차원, num_tokens: 토큰 개수, d_in: 입력 임베딩 크기\n",
        "\n",
        "        keys = self.W_key(x) # 크기: (b, num_tokens, d_out)\n",
        "        queries = self.W_query(x)\n",
        "        values = self.W_value(x)\n",
        "        print(\"key.shape before reshape:\", keys.shape)\n",
        "        print(\"b:\", b, \", num_tokens:\", num_tokens, \", d_out:\", self.d_out)\n",
        "        print()\n",
        "\n",
        "        # `num_heads` 차원을 추가함으로써 암묵적으로 행렬을 분할합니다.\n",
        "        # 그다음 마지막 차원을 `num_heads`에 맞춰 채웁니다: (b, num_tokens, d_out) -> (b, num_tokens, num_heads, head_dim)\n",
        "        keys = ### your code\n",
        "        values = ### your code\n",
        "        queries = ### your code\n",
        "        print(\"keys.shape after reshape:\", keys.shape)\n",
        "        print(\"d_out:\", self.d_out, \"= num_heads:\", self.num_heads, \"* head_dim:\", self.head_dim)\n",
        "        print()\n",
        "\n",
        "        # 전치: (b, num_tokens, num_heads, head_dim) -> (b, num_heads, num_tokens, head_dim)\n",
        "        keys = ### your code\n",
        "        queries = ### your code\n",
        "        values = ### your code\n",
        "        print(\"keys.shape after transpose:\", keys.shape)\n",
        "        print(\"b:\", b, \", num_heads:\", self.num_heads, \", num_tokens:\", num_tokens, \", head_dim:\", self.head_dim)\n",
        "        print()\n",
        "\n",
        "        # 코잘 마스크로 스케일드 점곱 어텐션(셀프 어텐션)을 계산합니다.\n",
        "        attn_scores = ### your code\n",
        "        print(\"attn_scores.shape:\", attn_scores.shape)\n",
        "        print(\"[6,2] @ [6,2]T = [6,6]\")\n",
        "        print()\n",
        "\n",
        "        # 마스크를 불리언 타입으로 만들고 토큰 개수로 마스크를 자릅니다.\n",
        "        mask_bool = ### your code\n",
        "\n",
        "        # 마스크를 사용해 어텐션 점수를 채웁니다.\n",
        "        ### your code\n",
        "\n",
        "        attn_weights = ### your code\n",
        "        attn_weights = ### your code\n",
        "\n",
        "        # 크기: (b, num_tokens, num_heads, head_dim)\n",
        "        context_vec = ### your code\n",
        "        print(\"context_vec.shape after transpose:\", context_vec.shape)\n",
        "        print(\"b:\", b, \", num_tokens:\", num_tokens, \", num_heads:\", self.num_heads, \", head_dim:\", self.head_dim)\n",
        "        print()\n",
        "\n",
        "        # 헤드를 결합합니다. self.d_out = self.num_heads * self.head_dim\n",
        "        context_vec = ### your code\n",
        "        context_vec = self.out_proj(context_vec) # 투영\n",
        "        print(\"context_vec.shape after reshape:\", context_vec.shape)\n",
        "        print(\"b:\" , b, \", num_tokens:\", num_tokens, \", d_out:\", self.d_out)\n",
        "        print()\n",
        "\n",
        "        return context_vec\n",
        "\n",
        "torch.manual_seed(123)\n",
        "\n",
        "batch_size, context_length, d_in = batch.shape\n",
        "d_out = 4\n",
        "mha = MultiHeadAttention(d_in, d_out, context_length, 0.0, num_heads=2)\n",
        "\n",
        "context_vecs = mha(batch)\n",
        "\n",
        "print(context_vecs)\n",
        "print(\"context_vecs.shape:\", context_vecs.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d334dfb5-2b6c-4c33-82d5-b4e9db5867bb",
      "metadata": {
        "id": "d334dfb5-2b6c-4c33-82d5-b4e9db5867bb"
      },
      "source": [
        "`MultiHeadAttention`는 `MultiHeadAttentionWrapper`를 더 효율적으로 재작성한 것입니다.  \n",
        "각 단계의 shape을 출력 결과를 통해 확인할 수 있습니다.\n",
        "\n",
        "`MultiHeadAttention`와 `MultiHeadAttentionWrapper`클래스는 모두 `d_out=2`를 사용하지만, 약간 다릅니다.  \n",
        "`MultiHeadAttentionWrapper`는 헤드의 출력을 연결하기 때문에 출력 차원이 `d_out * num_heads`입니다(즉, `2*2 = 4`).  \n",
        "하지만 `MultiHeadAttention` 클래스는 `num_heads`와 무관하게 출력 차원이 `d_out`입니다."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8b0ed78c-e8ac-4f8f-a479-a98242ae8f65",
      "metadata": {
        "id": "8b0ed78c-e8ac-4f8f-a479-a98242ae8f65"
      },
      "source": [
        "파이토치에서는 방금 구현한 멀티 헤드 어텐션 함수를 지원합니다.  \n",
        "더 효율적인 구현을 위해 [`torch.nn.MultiheadAttention`](https://pytorch.org/docs/stable/generated/torch.nn.MultiheadAttention.html) 클래스를 사용할 수 있습니다."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8ad2ffdf",
      "metadata": {
        "id": "8ad2ffdf"
      },
      "source": [
        "<table style=\"width:100%\">\n",
        "<tr>\n",
        "<td style=\"vertical-align:middle; text-align:left;\">\n",
        "<font size=\"2\">\n",
        "본 강의자료는 세바스찬 라시카(Sebastian Raschka)의 <a href=\"http://mng.bz/orYv\">Build a Large Language Model From Scratch</a> (<a href=\"<a href=\"http://tensorflow.blog/llm-from-scratch\">밑바닥부터 만들면서 배우는 LLM</a>)의 예제를 참고하여 제작되었습니다.\n",
        "</font>\n",
        "</td>\n",
        "</tr>\n",
        "</table>"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "llmfromscratch",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.19"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}